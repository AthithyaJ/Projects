{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-d66f7d9d1f66>:31: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\jayad\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-d66f7d9d1f66>:41: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "0 Last batch accuracy: 0.97 Test accuracy: 0.9751\n",
      "1 Last batch accuracy: 0.99 Test accuracy: 0.9837\n",
      "2 Last batch accuracy: 1.0 Test accuracy: 0.987\n",
      "3 Last batch accuracy: 0.97 Test accuracy: 0.9834\n",
      "4 Last batch accuracy: 1.0 Test accuracy: 0.9881\n",
      "5 Last batch accuracy: 1.0 Test accuracy: 0.9899\n",
      "6 Last batch accuracy: 1.0 Test accuracy: 0.9908\n",
      "7 Last batch accuracy: 1.0 Test accuracy: 0.9883\n",
      "8 Last batch accuracy: 1.0 Test accuracy: 0.9872\n",
      "9 Last batch accuracy: 1.0 Test accuracy: 0.9878\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Last batch accuracy:\", acc_batch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'fc1/kernel:0', 'fc1/bias:0', 'output/kernel:0', 'output/bias:0', 'conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'fc1/kernel:0', 'fc1/bias:0', 'output/kernel:0', 'output/bias:0', 'conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'fc1/kernel:0', 'fc1/bias:0', 'output/kernel:0', 'output/bias:0', 'conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'fc1/kernel:0', 'fc1/bias:0', 'output/kernel:0', 'output/bias:0']\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model\n",
      "(3, 3, 32, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'conv1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAAEKCAYAAADEq21yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADCBJREFUeJzt3XtsleUBBvDnbSl3KBdFYhFKkbpCRe73mw4FuUhADCJ4W5y6GTbZptM5zbIxsyxqplHMglucgDKwihFQQCgIgloioAIiIlC5lEkpKoJg229/9BibY0+ewxM8Rfv8kqY533nOd76WJ+85ffm+84YoimB2utJq+wDsh8nFMYmLYxIXxyQujklcHJO4OGeBEMKDIYTlIYTSEEIUQripto+JcXHODtMBNAKwuLYPJFn1avsADACQGUVRZQjhQgA31PbBJKPOjTghhEtCCC/GXhZOhBB2hBDujd0XQggzYttOhRAOhhAeDyE0j9tHFEKYGUL4VQhhdwjhixDCmhBC12qZWSGEQyGEenGPbRBCKAsh/OObbVEUVX7fP/eZVqeKE0LoC2ADgE4AZgAYA+ARAO1ikb/Gbq8AMA7A3wHcBGBJCCH+dzUt9vhfA7gZQHsAL1UryjMA2gC4Iu5xYwG0ADDnTP1ctSKKojrzBeB1AJ8AaFzDfa0AfAXg6bjt0wBEAK6qti0CsBNARrVtk2LbB1bb9iGA5+L2twjAtgTHd2FsHzfV9u+KfdWZESeE0BjAIADzoig6XkOkP4AGAObGbZ8PoBzAsLjtK6Io+rra7fdi39tX2zYXwPgQQrPYMbQCcCWqRqMftDpTHAAtUfXz7ktwf6vY94PVN0ZRVA6gtNr93zgSd/tk7HvDatvmxG5Pit2+FkAGgHlJH/VZqi4VpwxAJYCsBPd/U4S21TfG3rO0RlV5TksURbsBvIGqlzvEvq+OouiT093X2abOFCf28rQOwLQQQqMaIm+iatS4Nm77ZFRNW6wRn3oOgOEhhOEABuBH8DIF1L15nN+hqgAbQggPo+plKwdA9yiKpocQHgFwbwjhSwBLAeQBmImqwi0Rn3MBgMdQ9X7nBICC+EAIYRiAc/HtaNc7hHAMAKIoel583u9Xbb87T/UXgB4AXgZwFFX/kB8A+H3svoCqP9N3ADiFqvc7TwBoHrePCMDMuG3ZSPAXEYCFsfueTXBMq2P3f+ertn9fib5C7MDNTkudeY9jZ5aLYxIXxyQujklS+uf4Hx96ir4Tzzwnk+5n8OAeNPPTbj1pJj9/CM1MufMWmqmXkU4zAFBRzv8TfPOqzTQTQqCZrNxE85zfOrTnEM3Mfvy+Gp/MI45JXByTuDgmcXFM4uKYxMUxiYtjEhfHJCmdANy7dS/NTLxjPN/P4U9ppmP2xTST24VPJHbOy6aZ155fTTMAMHj8IJopO1RGM81aNqWZkM7HhA5dO9BMIh5xTOLimMTFMYmLYxIXxyQujklcHJO4OCZJ6eUxnTp1p0/WuXNvup/8Ad1ppknzxjQz+qqhNLN9T6JLzb9VejC5q4OXPcM/cKthQz6599ln/6OZrn160Uy7zvwswT/cNtVnANqZ4+KYxMUxiYtjEhfHJC6OSVwck7g4JknpGYAjJ1xHMx9ufp9mspKYuLr7xvhPZPuuTYW30szQq/kk4Rdlx2gGAHbseJtmbv7tXTQz99FHaWbfvh00M2DIWJrBbTVv9ohjEhfHJC6OSVwck7g4JnFxTOLimMTFMUlKJwC3bHiTZtq27Ugz+3fup5nevUbSzC/+wj/f78uTJ2nmw007aQYA0tL4ZwWuXMg/+X/EuMk0c6QkfnGb7+ozqg/NJOIRxyQujklcHJO4OCZxcUzi4pjExTGJi2OSlF4CvGrbVvpkJ78up/spWruFZkaPHkwzB44epZmVz/M1XO+cMZVmAOCDAwdpZuGsRTSTP5R/vmF6Op9s7N0zj2YG5eb6EmA7c1wck7g4JnFxTOLimMTFMYmLYxIXxyQpPQNw8fyVNHPsKL+ctrKCr6b77kV7+HMlceluvzH9aGbewuU0AwDd+nehmQvy2tPMy08V0MyQcZfRzKrSt2hmUG5ujds94pjExTGJi2MSF8ckLo5JXByTuDgmcXFM4uKYJKUzxz0v70kzlZV8VvjVp16lmVatMmlm02ubaKbwuUKaGfWzUTQDAJ9/eZxmSnaX0EyzZq1opnPvmmd8q9u2fhvNJOIRxyQujklcHJO4OCZxcUzi4pjExTGJi2OSlE4AZmTwp9u7vZhm6jesTzOL//0KzZSf4tepb9zIJxuTnQAs3raXZorW8gnH7Ox8mtn6xlaaGTiyL80k4hHHJC6OSVwck7g4JnFxTOLimMTFMYmLY5KUTgAumb2UZk4cO0EzU+/mS0O/ubyIZoq38cnGw4f30czHW3bRDAAsL+DXfE+983aaOfXVKZrJ7dGZ76ecT4Am4hHHJC6OSVwck7g4JnFxTOLimMTFMYmLY5KUTgBWJLEyTItzW9DMeZn88t5mLZvSzP5iPnF364w/00x5eQXNAEDni3rRTOPMJjRTvP0Tmtm15WOaueeBn9NMIh5xTOLimMTFMYmLYxIXxyQujklcHJO4OCZJ6QRgMitVdxvGl0ZeXcjP7rvv9mk006Z9G5pZv2gDzUyZPpFmAKC8gk8UvjB7Mc38a9YDNDNkyCSamV+wgmbuv+P6Grd7xDGJi2MSF8ckLo5JXByTuDgmcXFM4uKYJKUTgC3O5WfuNWjUgGZ+kpdDM7954DGaWVawgGaaNOHHvH1rd5oBkrt0t/RAKc3c//Bsmnl7OZ+4/DiJswQT8YhjEhfHJC6OSVwck7g4JnFxTOLimMTFMUlKJwD37eSXrg6cMIhmSo6U0cyIiUNpJjOJCcmykiM0s3IeP5MOAEZcfwXN5A/mC3xseOV1mrn5T7fRTOmBwzSTiEcck7g4JnFxTOLimMTFMYmLYxIXxyQujklSOgGY3ZWfuff+2vdo5t11m2nmqtsn0EzHbh1pJqNBBs0c2HWQZgDg/Oy2NFOvPv8n6TmsH82sWbCGZi6dMpxmEvGIYxIXxyQujklcHJO4OCZxcUzi4pjExTFJSicAmyaxMEd21w40s3T+f2lm12a+wEeTFnzBjXdWbKSZK24cSTMAcOyL4zTTIz+XZoq37aWZdrntaOaf9z5OM9cWDqhxu0cck7g4JnFxTOLimMTFMYmLYxIXxyQujklSOgF46ZiBNLOsYDXNtG+fRzObXn+LZvqP4pcJXzK8B82se+ENmgGA7pfxzwr8aNNHNJOWnk4zzVs3p5kGDRrRTMJjkB9pdZqLYxIXxyQujklcHJO4OCZxcUzi4pgkpROArZvyMwBL9/NFMKIklhO+evoUmqko56vyluwuoZms3CyaAYAXZ8+lmYGXj6CZvmP4JcAh8OPJSeJsw0Q84pjExTGJi2MSF8ckLo5JXByTuDgmcXFMEpKZTDtTVm7dSp8sq2VLup8XXi6kmfw+/CzBJ+7hl8B+tOsdmunShZ/ZCAADxvIFTho25WflXXAhn3Bc99J6mlm/gi9eUlS0tMapRI84JnFxTOLimMTFMYmLYxIXxyQujklcHJO4OCZJ6amjh45+RjNPPzSfZg4U76GZNh3Oo5m8PhfTTKf8i2hmScEzNAMAQyYOp5msTufTTLNGfHa5RRJLY+/d8z7NJOIRxyQujklcHJO4OCZxcUzi4pjExTGJi2OSlE4Azpn5NM2Ulu6nmaKipTQz4ZeTaWbEpOE085+/PUsz6enJ/Ro3LuMr0eR0zaaZkk+P0ExaPf4Bk336jqaZhPuXH2l1motjEhfHJC6OSVwck7g4JnFxTOLimCSlE4D9ruTXWM+f9STN5ORcQjOLnlxAM11689Vc+o/rTzOZ5/Cz7QCgMokPq7xr8i00M+aaG2im+AO+xHTrtm1oJhGPOCZxcUzi4pjExTGJi2MSF8ckLo5JXByTpHQC8JqJfGWUtDTe5cKCZTST14tPEjY/hy+xPKBfN76fJJZqBoALsvhlySGNL/uSUZ//s/W4rCfNHP/8OM0k4hHHJC6OSVwck7g4JnFxTOLimMTFMYmLY5KUTgAWLFpFMx26tKeZ63JupJm1BetopkmLJjSzZjW/bHf86KE0AwBFO3bSTNmhozRTWcFX/Bk75XKa2bOPL42diEcck7g4JnFxTOLimMTFMYmLYxIXxyQujklSuny0/Xh4xDGJi2MSF8ckLo5JXByTuDgmcXFM4uKYxMUxiYtjEhfHJC6OSVwck7g4JnFxTOLimMTFMYmLYxIXxyQujklcHJO4OCZxcUzyf1rpCdVG5rTpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_mnist_model.meta\")  \n",
    "graph = tf.get_default_graph()\n",
    "operations = graph.get_operations();\n",
    "print([v.name for v in tf.trainable_variables()])  # those parameters learned\n",
    "ker1 = graph.get_tensor_by_name(\"conv1/kernel:0\")  # first convolutional layer\n",
    "ker2 = graph.get_tensor_by_name(\"conv2/kernel:0\")  # second convolutional layer\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, tf.train.latest_checkpoint(\"./\"))  # the learned parameters\n",
    "features1 = sess.run(ker1)  # get weights of the first convolutional layer\n",
    "features2 = sess.run(ker2)  \n",
    "print(features2.shape)  # how many values are in the second convolutional layer!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digits(instances, images_per_row=4, **options):  # borrowed from DL1B.py\n",
    "   # instances should be an array of 32 arrays of size 9 each for the 32 conv1 features\n",
    "    size = 3\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = 'bone')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(features1.flatten().reshape(3,3,32).transpose())  # you fill the new shape\n",
    "plt.title(\"conv1\", fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Last batch accuracy: 0.86 Test accuracy: 0.874\n",
      "1 Last batch accuracy: 0.89 Test accuracy: 0.8846\n",
      "2 Last batch accuracy: 0.92 Test accuracy: 0.8915\n",
      "3 Last batch accuracy: 0.91 Test accuracy: 0.8952\n",
      "4 Last batch accuracy: 0.95 Test accuracy: 0.8908\n",
      "5 Last batch accuracy: 0.93 Test accuracy: 0.907\n",
      "6 Last batch accuracy: 0.94 Test accuracy: 0.91\n",
      "7 Last batch accuracy: 0.92 Test accuracy: 0.9107\n",
      "8 Last batch accuracy: 0.97 Test accuracy: 0.9104\n",
      "9 Last batch accuracy: 0.98 Test accuracy: 0.9131\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Last batch accuracy:\", acc_batch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_fashion_mnist_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'fc1/kernel:0', 'fc1/bias:0', 'output/kernel:0', 'output/bias:0', 'conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'fc1/kernel:0', 'fc1/bias:0', 'output/kernel:0', 'output/bias:0']\n",
      "INFO:tensorflow:Restoring parameters from ./my_fashion_mnist_model\n",
      "(3, 3, 32, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'conv1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_fashion_mnist_model.meta\")  \n",
    "graph = tf.get_default_graph()\n",
    "operations = graph.get_operations();\n",
    "print([v.name for v in tf.trainable_variables()])  # those parameters learned\n",
    "ker1 = graph.get_tensor_by_name(\"conv1/kernel:0\")  # first convolutional layer\n",
    "ker2 = graph.get_tensor_by_name(\"conv2/kernel:0\")  # second convolutional layer\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, tf.train.latest_checkpoint(\"./\"))  # the learned parameters\n",
    "features1 = sess.run(ker1)  # get weights of the first convolutional layer\n",
    "features2 = sess.run(ker2)  \n",
    "print(features2.shape)  # how many values are in the second convolutional layer!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digits(instances, images_per_row=4, **options):  # borrowed from DL1B.py\n",
    "   # instances should be an array of 32 arrays of size 9 each for the 32 conv1 features\n",
    "    size = 3\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = 'bone')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(features1.flatten().reshape(3,3,32).transpose())  # you fill the new shape\n",
    "plt.title(\"conv1\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
